{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25375,"status":"ok","timestamp":1671046189553,"user":{"displayName":"Jorge Gutiérrez","userId":"17399610914182888435"},"user_tz":180},"id":"DyCHvkalD3Ut","outputId":"85c4ec54-92e4-4697-9076-ae90fddce3a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":8896,"status":"ok","timestamp":1671046198443,"user":{"displayName":"Jorge Gutiérrez","userId":"17399610914182888435"},"user_tz":180},"id":"uMOEnL__CIfa"},"outputs":[],"source":["import drive.MyDrive.Proyecto.project_utils as pu\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import torch\n","from torch.utils.data import DataLoader\n","import torch.nn as nn"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1671046198444,"user":{"displayName":"Jorge Gutiérrez","userId":"17399610914182888435"},"user_tz":180},"id":"6cd1lnXwi_Q0"},"outputs":[],"source":["BATCH_SIZE = 64"]},{"cell_type":"markdown","metadata":{"id":"3wElshxVjHzB"},"source":["# Modelos"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":1373,"status":"ok","timestamp":1671046199813,"user":{"displayName":"Jorge Gutiérrez","userId":"17399610914182888435"},"user_tz":180},"id":"SV9rrUkpizdj"},"outputs":[],"source":["import drive.MyDrive.Proyecto.models as mdls"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1671046199813,"user":{"displayName":"Jorge Gutiérrez","userId":"17399610914182888435"},"user_tz":180},"id":"6q7JfC26mOlJ"},"outputs":[],"source":["DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","CHANNELS = 39\n","BASEPATH = \"/content/drive/MyDrive/Proyecto/Datasets/\"\n","pu.iniciar_semillas()"]},{"cell_type":"code","source":["windows = [20, 40, 80, 120]\n","bidirecc = [True, False]\n","data= [\n","    #(learning_rate, epochs, hidden_size, num_layers, modelo)\n","    (2e-4, 100, 256, 1, mdls.RNN),\n","    (2e-4, 100, 64, 2, mdls.GRU),\n","    (2e-4, 100, 64, 2, mdls.LSTM),\n","]\n","BATCH_SIZE = 64\n","\n","for window in windows:\n","  train_df = pd.read_pickle(BASEPATH + f'train_{window}ms_win.pkl')\n","  val_df = pd.read_pickle(BASEPATH + f'val_{window}ms_win.pkl')\n","  train_df = pu.oversample(train_df)\n","  max_length = max(\n","      train_df.mfcc.apply(lambda x: x[0].size).max(),\n","      train_df.delta.apply(lambda x: x[0].size).max(),\n","      val_df.mfcc.apply(lambda x: x[0].size).max(),\n","      val_df.delta.apply(lambda x: x[0].size).max()\n","  )\n","  categories, c2i, i2c = pu.get_categories(train_df)\n","  NUM_CLASSES = len(categories)\n","  train_df, test_df = train_test_split(train_df, test_size=0.15)\n","  \n","  train_data = pu.SCData(train_df, categories, c2i, i2c, max_length)\n","  test_data = pu.SCData(test_df, categories, c2i, i2c, max_length)\n","  valid_data = pu.SCData(val_df, categories, c2i, i2c, max_length)\n","\n","  train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n","  test_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n","  valid_loader = DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=True)\n","  for bidirec in bidirecc:\n","    for datum in data:\n","      lr, epochs, hs, n_lay, cls = datum\n","      name = cls.__name__\n","      desc = f'_{window}ms_{bidirec}'\n","      model_name = f'{name}_{desc}'\n","      model = cls(hs, n_lay, NUM_CLASSES, CHANNELS, max_length, bidirec)\n","      criterion = nn.CrossEntropyLoss()\n","      optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)  \n","\n","      train_loss_evolution, train_acc_evolution, val_acc_evolution, val_loss_evolution = pu.train_complete(model, criterion, optimizer, epochs, train_loader, valid_loader, model_name, DEVICE, False)\n","\n","      model = cls(hs, n_lay, NUM_CLASSES, CHANNELS, max_length, bidirec)\n","      model.load_state_dict(torch.load(f'{model_name}.pt'))\n","\n","      # pu.plot_train_evo(train_loss_evolution, val_loss_evolution, train_acc_evolution, val_acc_evolution, name, desc)\n","      # pu.plot_conf_matrix(model, valid_loader, categories, name, desc, DEVICE)\n","\n"],"metadata":{"id":"xZqZafv7xi1R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671046655911,"user_tz":180,"elapsed":374403,"user":{"displayName":"Jorge Gutiérrez","userId":"17399610914182888435"}},"outputId":"2a85e0ef-c283-496a-cf94-dc378d91ef55"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 13238/13238 [00:02<00:00, 4739.36it/s]\n","100%|██████████| 2337/2337 [00:00<00:00, 5364.19it/s]\n","100%|██████████| 9981/9981 [00:02<00:00, 4142.59it/s]\n","/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Epoch Time: 0m 2s\n","\tTrain Loss: 3.549 | Train Acc: 3.83%\n","\t Val. Loss: 3.552 |  Val. Acc: 3.42%\n","Epoch: 02 | Epoch Time: 0m 2s\n","\tTrain Loss: 3.533 | Train Acc: 4.53%\n","\t Val. Loss: 3.555 |  Val. Acc: 2.51%\n","Epoch: 03 | Epoch Time: 0m 2s\n","\tTrain Loss: 3.516 | Train Acc: 5.01%\n","\t Val. Loss: 3.538 |  Val. Acc: 4.30%\n","Epoch: 04 | Epoch Time: 0m 2s\n","\tTrain Loss: 3.519 | Train Acc: 5.61%\n","\t Val. Loss: 3.552 |  Val. Acc: 3.68%\n","Epoch: 05 | Epoch Time: 0m 2s\n","\tTrain Loss: 3.481 | Train Acc: 6.44%\n","\t Val. Loss: 3.525 |  Val. Acc: 3.84%\n","Epoch: 06 | Epoch Time: 0m 2s\n","\tTrain Loss: 3.414 | Train Acc: 7.76%\n","\t Val. Loss: 3.459 |  Val. Acc: 4.78%\n","Epoch: 07 | Epoch Time: 0m 2s\n","\tTrain Loss: 3.301 | Train Acc: 9.71%\n","\t Val. Loss: 3.351 |  Val. Acc: 6.64%\n","Epoch: 08 | Epoch Time: 0m 2s\n","\tTrain Loss: 3.317 | Train Acc: 8.24%\n","\t Val. Loss: 3.355 |  Val. Acc: 6.59%\n","Epoch: 09 | Epoch Time: 0m 2s\n","\tTrain Loss: 3.296 | Train Acc: 9.14%\n","\t Val. Loss: 3.351 |  Val. Acc: 7.17%\n","Epoch: 10 | Epoch Time: 0m 2s\n","\tTrain Loss: 3.214 | Train Acc: 10.82%\n","\t Val. Loss: 3.285 |  Val. Acc: 7.40%\n","Test Loss: 3.284 | Mejor test acc: 7.44%\n","Epoch: 01 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.419 | Train Acc: 7.62%\n","\t Val. Loss: 3.434 |  Val. Acc: 6.14%\n","Epoch: 02 | Epoch Time: 0m 2s\n","\tTrain Loss: 3.052 | Train Acc: 14.17%\n","\t Val. Loss: 3.061 |  Val. Acc: 12.65%\n","Epoch: 03 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.792 | Train Acc: 19.07%\n","\t Val. Loss: 2.816 |  Val. Acc: 18.07%\n","Epoch: 04 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.618 | Train Acc: 23.23%\n","\t Val. Loss: 2.658 |  Val. Acc: 20.97%\n","Epoch: 05 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.429 | Train Acc: 30.37%\n","\t Val. Loss: 2.492 |  Val. Acc: 26.62%\n","Epoch: 06 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.263 | Train Acc: 34.07%\n","\t Val. Loss: 2.330 |  Val. Acc: 30.71%\n","Epoch: 07 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.099 | Train Acc: 40.14%\n","\t Val. Loss: 2.179 |  Val. Acc: 35.68%\n","Epoch: 08 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.938 | Train Acc: 45.20%\n","\t Val. Loss: 2.022 |  Val. Acc: 41.12%\n","Epoch: 09 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.777 | Train Acc: 50.95%\n","\t Val. Loss: 1.864 |  Val. Acc: 46.45%\n","Epoch: 10 | Epoch Time: 0m 2s\n","\tTrain Loss: 1.614 | Train Acc: 55.75%\n","\t Val. Loss: 1.718 |  Val. Acc: 50.34%\n","Test Loss: 1.718 | Mejor test acc: 50.41%\n","Epoch: 01 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.373 | Train Acc: 6.52%\n","\t Val. Loss: 3.357 |  Val. Acc: 6.45%\n","Epoch: 02 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.195 | Train Acc: 9.21%\n","\t Val. Loss: 3.197 |  Val. Acc: 7.93%\n","Epoch: 03 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.084 | Train Acc: 12.62%\n","\t Val. Loss: 3.101 |  Val. Acc: 11.08%\n","Epoch: 04 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.987 | Train Acc: 14.99%\n","\t Val. Loss: 3.023 |  Val. Acc: 12.67%\n","Epoch: 05 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.868 | Train Acc: 17.28%\n","\t Val. Loss: 2.922 |  Val. Acc: 15.15%\n","Epoch: 06 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.867 | Train Acc: 16.22%\n","\t Val. Loss: 2.913 |  Val. Acc: 13.78%\n","Epoch: 07 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.829 | Train Acc: 18.36%\n","\t Val. Loss: 2.892 |  Val. Acc: 16.94%\n","Epoch: 08 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.906 | Train Acc: 15.87%\n","\t Val. Loss: 2.975 |  Val. Acc: 13.34%\n","Epoch: 09 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.518 | Train Acc: 26.29%\n","\t Val. Loss: 2.627 |  Val. Acc: 21.37%\n","Epoch: 10 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.474 | Train Acc: 26.08%\n","\t Val. Loss: 2.576 |  Val. Acc: 22.32%\n","Test Loss: 2.579 | Mejor test acc: 22.03%\n","Epoch: 01 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.528 | Train Acc: 4.04%\n","\t Val. Loss: 3.559 |  Val. Acc: 2.24%\n","Epoch: 02 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.485 | Train Acc: 5.53%\n","\t Val. Loss: 3.513 |  Val. Acc: 4.59%\n","Epoch: 03 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.588 | Train Acc: 3.08%\n","\t Val. Loss: 3.569 |  Val. Acc: 4.20%\n","Epoch: 04 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.416 | Train Acc: 6.01%\n","\t Val. Loss: 3.435 |  Val. Acc: 5.45%\n","Epoch: 05 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.479 | Train Acc: 6.55%\n","\t Val. Loss: 3.518 |  Val. Acc: 4.43%\n","Epoch: 06 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.337 | Train Acc: 8.76%\n","\t Val. Loss: 3.364 |  Val. Acc: 5.87%\n","Epoch: 07 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.440 | Train Acc: 6.94%\n","\t Val. Loss: 3.404 |  Val. Acc: 7.41%\n","Epoch: 08 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.406 | Train Acc: 6.84%\n","\t Val. Loss: 3.441 |  Val. Acc: 5.20%\n","Epoch: 09 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.449 | Train Acc: 6.40%\n","\t Val. Loss: 3.484 |  Val. Acc: 4.19%\n","Epoch: 10 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.280 | Train Acc: 9.14%\n","\t Val. Loss: 3.315 |  Val. Acc: 7.44%\n","Test Loss: 3.316 | Mejor test acc: 7.26%\n","Epoch: 01 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.300 | Train Acc: 6.78%\n","\t Val. Loss: 3.293 |  Val. Acc: 6.45%\n","Epoch: 02 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.110 | Train Acc: 11.54%\n","\t Val. Loss: 3.119 |  Val. Acc: 9.75%\n","Epoch: 03 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.968 | Train Acc: 15.07%\n","\t Val. Loss: 2.994 |  Val. Acc: 12.96%\n","Epoch: 04 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.747 | Train Acc: 20.98%\n","\t Val. Loss: 2.783 |  Val. Acc: 16.90%\n","Epoch: 05 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.531 | Train Acc: 26.64%\n","\t Val. Loss: 2.583 |  Val. Acc: 22.50%\n","Epoch: 06 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.372 | Train Acc: 31.68%\n","\t Val. Loss: 2.432 |  Val. Acc: 28.00%\n","Epoch: 07 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.244 | Train Acc: 34.91%\n","\t Val. Loss: 2.315 |  Val. Acc: 30.96%\n","Epoch: 08 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.128 | Train Acc: 39.07%\n","\t Val. Loss: 2.205 |  Val. Acc: 34.07%\n","Epoch: 09 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.037 | Train Acc: 41.19%\n","\t Val. Loss: 2.125 |  Val. Acc: 36.27%\n","Epoch: 10 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.932 | Train Acc: 44.34%\n","\t Val. Loss: 2.025 |  Val. Acc: 39.45%\n","Test Loss: 2.025 | Mejor test acc: 39.30%\n","Epoch: 01 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.399 | Train Acc: 7.18%\n","\t Val. Loss: 3.417 |  Val. Acc: 5.17%\n","Epoch: 02 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.218 | Train Acc: 9.65%\n","\t Val. Loss: 3.250 |  Val. Acc: 7.71%\n","Epoch: 03 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.179 | Train Acc: 9.93%\n","\t Val. Loss: 3.225 |  Val. Acc: 7.80%\n","Epoch: 04 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.961 | Train Acc: 14.54%\n","\t Val. Loss: 2.993 |  Val. Acc: 13.31%\n","Epoch: 05 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.810 | Train Acc: 18.03%\n","\t Val. Loss: 2.854 |  Val. Acc: 16.28%\n","Epoch: 06 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.733 | Train Acc: 20.33%\n","\t Val. Loss: 2.784 |  Val. Acc: 17.95%\n","Epoch: 07 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.656 | Train Acc: 22.96%\n","\t Val. Loss: 2.697 |  Val. Acc: 20.77%\n","Epoch: 08 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.594 | Train Acc: 23.71%\n","\t Val. Loss: 2.647 |  Val. Acc: 20.82%\n","Epoch: 09 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.496 | Train Acc: 27.71%\n","\t Val. Loss: 2.567 |  Val. Acc: 23.71%\n","Epoch: 10 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.420 | Train Acc: 30.27%\n","\t Val. Loss: 2.500 |  Val. Acc: 25.63%\n","Test Loss: 2.502 | Mejor test acc: 25.82%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 13238/13238 [00:02<00:00, 5810.01it/s]\n","100%|██████████| 2337/2337 [00:00<00:00, 5893.84it/s]\n","100%|██████████| 9981/9981 [00:01<00:00, 5179.92it/s]\n","/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.335 | Train Acc: 8.16%\n","\t Val. Loss: 3.364 |  Val. Acc: 6.10%\n","Epoch: 02 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.211 | Train Acc: 10.64%\n","\t Val. Loss: 3.238 |  Val. Acc: 8.99%\n","Epoch: 03 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.072 | Train Acc: 13.42%\n","\t Val. Loss: 3.106 |  Val. Acc: 11.12%\n","Epoch: 04 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.988 | Train Acc: 14.51%\n","\t Val. Loss: 3.021 |  Val. Acc: 12.19%\n","Epoch: 05 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.841 | Train Acc: 18.60%\n","\t Val. Loss: 2.908 |  Val. Acc: 14.29%\n","Epoch: 06 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.698 | Train Acc: 20.54%\n","\t Val. Loss: 2.778 |  Val. Acc: 16.36%\n","Epoch: 07 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.699 | Train Acc: 20.82%\n","\t Val. Loss: 2.763 |  Val. Acc: 16.81%\n","Epoch: 08 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.583 | Train Acc: 23.72%\n","\t Val. Loss: 2.628 |  Val. Acc: 19.83%\n","Epoch: 09 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.435 | Train Acc: 27.52%\n","\t Val. Loss: 2.517 |  Val. Acc: 22.74%\n","Epoch: 10 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.363 | Train Acc: 29.54%\n","\t Val. Loss: 2.469 |  Val. Acc: 23.95%\n","Test Loss: 2.467 | Mejor test acc: 24.32%\n","Epoch: 01 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.282 | Train Acc: 7.88%\n","\t Val. Loss: 3.284 |  Val. Acc: 6.56%\n","Epoch: 02 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.057 | Train Acc: 13.59%\n","\t Val. Loss: 3.055 |  Val. Acc: 12.58%\n","Epoch: 03 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.710 | Train Acc: 22.08%\n","\t Val. Loss: 2.724 |  Val. Acc: 20.86%\n","Epoch: 04 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.431 | Train Acc: 30.62%\n","\t Val. Loss: 2.468 |  Val. Acc: 28.40%\n","Epoch: 05 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.193 | Train Acc: 38.70%\n","\t Val. Loss: 2.236 |  Val. Acc: 35.44%\n","Epoch: 06 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.988 | Train Acc: 43.74%\n","\t Val. Loss: 2.029 |  Val. Acc: 41.56%\n","Epoch: 07 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.812 | Train Acc: 48.56%\n","\t Val. Loss: 1.870 |  Val. Acc: 45.63%\n","Epoch: 08 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.650 | Train Acc: 54.00%\n","\t Val. Loss: 1.716 |  Val. Acc: 51.12%\n","Epoch: 09 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.513 | Train Acc: 58.12%\n","\t Val. Loss: 1.594 |  Val. Acc: 54.86%\n","Epoch: 10 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.367 | Train Acc: 61.62%\n","\t Val. Loss: 1.468 |  Val. Acc: 57.92%\n","Test Loss: 1.469 | Mejor test acc: 57.99%\n","Epoch: 01 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.230 | Train Acc: 10.46%\n","\t Val. Loss: 3.242 |  Val. Acc: 9.01%\n","Epoch: 02 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.907 | Train Acc: 18.11%\n","\t Val. Loss: 2.947 |  Val. Acc: 15.36%\n","Epoch: 03 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.683 | Train Acc: 24.92%\n","\t Val. Loss: 2.731 |  Val. Acc: 21.54%\n","Epoch: 04 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.449 | Train Acc: 32.40%\n","\t Val. Loss: 2.526 |  Val. Acc: 28.00%\n","Epoch: 05 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.275 | Train Acc: 36.71%\n","\t Val. Loss: 2.366 |  Val. Acc: 32.06%\n","Epoch: 06 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.114 | Train Acc: 40.36%\n","\t Val. Loss: 2.232 |  Val. Acc: 34.96%\n","Epoch: 07 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.995 | Train Acc: 42.76%\n","\t Val. Loss: 2.152 |  Val. Acc: 37.25%\n","Epoch: 08 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.833 | Train Acc: 47.66%\n","\t Val. Loss: 2.000 |  Val. Acc: 41.55%\n","Epoch: 09 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.860 | Train Acc: 45.50%\n","\t Val. Loss: 2.012 |  Val. Acc: 40.55%\n","Epoch: 10 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.634 | Train Acc: 53.25%\n","\t Val. Loss: 1.812 |  Val. Acc: 46.94%\n","Test Loss: 1.813 | Mejor test acc: 46.78%\n","Epoch: 01 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.319 | Train Acc: 7.82%\n","\t Val. Loss: 3.340 |  Val. Acc: 5.35%\n","Epoch: 02 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.255 | Train Acc: 9.33%\n","\t Val. Loss: 3.282 |  Val. Acc: 7.15%\n","Epoch: 03 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.440 | Train Acc: 7.23%\n","\t Val. Loss: 3.460 |  Val. Acc: 5.49%\n","Epoch: 04 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.108 | Train Acc: 12.27%\n","\t Val. Loss: 3.142 |  Val. Acc: 9.35%\n","Epoch: 05 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.127 | Train Acc: 10.89%\n","\t Val. Loss: 3.181 |  Val. Acc: 8.30%\n","Epoch: 06 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.807 | Train Acc: 18.89%\n","\t Val. Loss: 2.859 |  Val. Acc: 16.38%\n","Epoch: 07 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.786 | Train Acc: 19.98%\n","\t Val. Loss: 2.837 |  Val. Acc: 16.11%\n","Epoch: 08 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.748 | Train Acc: 20.77%\n","\t Val. Loss: 2.805 |  Val. Acc: 17.44%\n","Epoch: 09 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.111 | Train Acc: 15.57%\n","\t Val. Loss: 3.111 |  Val. Acc: 14.70%\n","Epoch: 10 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.699 | Train Acc: 20.49%\n","\t Val. Loss: 2.769 |  Val. Acc: 18.50%\n","Test Loss: 2.766 | Mejor test acc: 18.64%\n","Epoch: 01 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.357 | Train Acc: 7.02%\n","\t Val. Loss: 3.360 |  Val. Acc: 6.34%\n","Epoch: 02 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.176 | Train Acc: 10.89%\n","\t Val. Loss: 3.187 |  Val. Acc: 10.26%\n","Epoch: 03 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.880 | Train Acc: 17.13%\n","\t Val. Loss: 2.903 |  Val. Acc: 15.92%\n","Epoch: 04 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.680 | Train Acc: 22.50%\n","\t Val. Loss: 2.698 |  Val. Acc: 22.27%\n","Epoch: 05 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.526 | Train Acc: 26.10%\n","\t Val. Loss: 2.538 |  Val. Acc: 25.15%\n","Epoch: 06 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.377 | Train Acc: 29.68%\n","\t Val. Loss: 2.403 |  Val. Acc: 28.06%\n","Epoch: 07 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.254 | Train Acc: 32.62%\n","\t Val. Loss: 2.301 |  Val. Acc: 30.47%\n","Epoch: 08 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.121 | Train Acc: 37.22%\n","\t Val. Loss: 2.167 |  Val. Acc: 33.86%\n","Epoch: 09 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.994 | Train Acc: 40.93%\n","\t Val. Loss: 2.055 |  Val. Acc: 37.00%\n","Epoch: 10 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.870 | Train Acc: 45.04%\n","\t Val. Loss: 1.931 |  Val. Acc: 42.50%\n","Test Loss: 1.932 | Mejor test acc: 42.42%\n","Epoch: 01 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.291 | Train Acc: 7.30%\n","\t Val. Loss: 3.285 |  Val. Acc: 7.44%\n","Epoch: 02 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.082 | Train Acc: 13.45%\n","\t Val. Loss: 3.083 |  Val. Acc: 12.20%\n","Epoch: 03 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.820 | Train Acc: 20.15%\n","\t Val. Loss: 2.832 |  Val. Acc: 18.95%\n","Epoch: 04 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.621 | Train Acc: 25.21%\n","\t Val. Loss: 2.659 |  Val. Acc: 22.96%\n","Epoch: 05 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.438 | Train Acc: 30.94%\n","\t Val. Loss: 2.509 |  Val. Acc: 27.45%\n","Epoch: 06 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.270 | Train Acc: 35.23%\n","\t Val. Loss: 2.361 |  Val. Acc: 31.74%\n","Epoch: 07 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.141 | Train Acc: 39.09%\n","\t Val. Loss: 2.249 |  Val. Acc: 34.34%\n","Epoch: 08 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.028 | Train Acc: 41.70%\n","\t Val. Loss: 2.161 |  Val. Acc: 37.01%\n","Epoch: 09 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.886 | Train Acc: 46.16%\n","\t Val. Loss: 2.058 |  Val. Acc: 40.14%\n","Epoch: 10 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.789 | Train Acc: 48.67%\n","\t Val. Loss: 1.964 |  Val. Acc: 42.78%\n","Test Loss: 1.967 | Mejor test acc: 42.69%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 13238/13238 [00:02<00:00, 6017.89it/s]\n","100%|██████████| 2337/2337 [00:00<00:00, 6036.61it/s]\n","100%|██████████| 9981/9981 [00:02<00:00, 4161.90it/s]\n","/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.039 | Train Acc: 13.74%\n","\t Val. Loss: 3.059 |  Val. Acc: 11.64%\n","Epoch: 02 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.809 | Train Acc: 18.69%\n","\t Val. Loss: 2.886 |  Val. Acc: 14.92%\n","Epoch: 03 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.702 | Train Acc: 21.38%\n","\t Val. Loss: 2.783 |  Val. Acc: 17.94%\n","Epoch: 04 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.592 | Train Acc: 22.58%\n","\t Val. Loss: 2.637 |  Val. Acc: 20.57%\n","Epoch: 05 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.447 | Train Acc: 27.03%\n","\t Val. Loss: 2.542 |  Val. Acc: 22.69%\n","Epoch: 06 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.326 | Train Acc: 30.12%\n","\t Val. Loss: 2.419 |  Val. Acc: 24.69%\n","Epoch: 07 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.394 | Train Acc: 26.47%\n","\t Val. Loss: 2.407 |  Val. Acc: 23.94%\n","Epoch: 08 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.125 | Train Acc: 36.08%\n","\t Val. Loss: 2.232 |  Val. Acc: 30.61%\n","Epoch: 09 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.132 | Train Acc: 35.36%\n","\t Val. Loss: 2.217 |  Val. Acc: 30.81%\n","Epoch: 10 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.119 | Train Acc: 36.17%\n","\t Val. Loss: 2.306 |  Val. Acc: 29.01%\n","Test Loss: 2.220 | Mejor test acc: 30.80%\n","Epoch: 01 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.113 | Train Acc: 16.79%\n","\t Val. Loss: 3.118 |  Val. Acc: 14.17%\n","Epoch: 02 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.638 | Train Acc: 24.02%\n","\t Val. Loss: 2.669 |  Val. Acc: 20.88%\n","Epoch: 03 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.381 | Train Acc: 31.33%\n","\t Val. Loss: 2.425 |  Val. Acc: 27.22%\n","Epoch: 04 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.129 | Train Acc: 40.43%\n","\t Val. Loss: 2.204 |  Val. Acc: 35.94%\n","Epoch: 05 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.893 | Train Acc: 48.38%\n","\t Val. Loss: 1.972 |  Val. Acc: 43.42%\n","Epoch: 06 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.697 | Train Acc: 53.32%\n","\t Val. Loss: 1.786 |  Val. Acc: 48.37%\n","Epoch: 07 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.522 | Train Acc: 58.15%\n","\t Val. Loss: 1.624 |  Val. Acc: 52.64%\n","Epoch: 08 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.375 | Train Acc: 61.79%\n","\t Val. Loss: 1.476 |  Val. Acc: 57.19%\n","Epoch: 09 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.251 | Train Acc: 64.74%\n","\t Val. Loss: 1.380 |  Val. Acc: 59.80%\n","Epoch: 10 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.134 | Train Acc: 68.60%\n","\t Val. Loss: 1.283 |  Val. Acc: 63.30%\n","Test Loss: 1.284 | Mejor test acc: 63.21%\n","Epoch: 01 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.981 | Train Acc: 15.47%\n","\t Val. Loss: 3.009 |  Val. Acc: 13.47%\n","Epoch: 02 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.653 | Train Acc: 22.58%\n","\t Val. Loss: 2.701 |  Val. Acc: 19.29%\n","Epoch: 03 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.372 | Train Acc: 31.75%\n","\t Val. Loss: 2.470 |  Val. Acc: 25.98%\n","Epoch: 04 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.145 | Train Acc: 38.57%\n","\t Val. Loss: 2.249 |  Val. Acc: 33.61%\n","Epoch: 05 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.951 | Train Acc: 45.40%\n","\t Val. Loss: 2.087 |  Val. Acc: 39.01%\n","Epoch: 06 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.795 | Train Acc: 49.55%\n","\t Val. Loss: 1.944 |  Val. Acc: 42.98%\n","Epoch: 07 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.603 | Train Acc: 55.09%\n","\t Val. Loss: 1.775 |  Val. Acc: 47.74%\n","Epoch: 08 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.477 | Train Acc: 58.64%\n","\t Val. Loss: 1.682 |  Val. Acc: 50.39%\n","Epoch: 09 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.310 | Train Acc: 64.05%\n","\t Val. Loss: 1.519 |  Val. Acc: 55.48%\n","Epoch: 10 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.195 | Train Acc: 66.86%\n","\t Val. Loss: 1.412 |  Val. Acc: 58.68%\n","Test Loss: 1.412 | Mejor test acc: 58.74%\n","Epoch: 01 | Epoch Time: 0m 0s\n","\tTrain Loss: 3.025 | Train Acc: 15.40%\n","\t Val. Loss: 3.069 |  Val. Acc: 11.94%\n","Epoch: 02 | Epoch Time: 0m 0s\n","\tTrain Loss: 2.766 | Train Acc: 19.80%\n","\t Val. Loss: 2.825 |  Val. Acc: 16.88%\n","Epoch: 03 | Epoch Time: 0m 0s\n","\tTrain Loss: 2.644 | Train Acc: 23.46%\n","\t Val. Loss: 2.716 |  Val. Acc: 18.51%\n","Epoch: 04 | Epoch Time: 0m 0s\n","\tTrain Loss: 2.526 | Train Acc: 26.27%\n","\t Val. Loss: 2.610 |  Val. Acc: 23.21%\n","Epoch: 05 | Epoch Time: 0m 0s\n","\tTrain Loss: 2.415 | Train Acc: 27.18%\n","\t Val. Loss: 2.510 |  Val. Acc: 23.71%\n","Epoch: 06 | Epoch Time: 0m 0s\n","\tTrain Loss: 2.340 | Train Acc: 29.68%\n","\t Val. Loss: 2.456 |  Val. Acc: 26.78%\n","Epoch: 07 | Epoch Time: 0m 0s\n","\tTrain Loss: 2.214 | Train Acc: 33.25%\n","\t Val. Loss: 2.353 |  Val. Acc: 28.53%\n","Epoch: 08 | Epoch Time: 0m 0s\n","\tTrain Loss: 2.029 | Train Acc: 38.65%\n","\t Val. Loss: 2.150 |  Val. Acc: 33.77%\n","Epoch: 09 | Epoch Time: 0m 0s\n","\tTrain Loss: 2.133 | Train Acc: 35.53%\n","\t Val. Loss: 2.275 |  Val. Acc: 31.02%\n","Epoch: 10 | Epoch Time: 0m 0s\n","\tTrain Loss: 2.136 | Train Acc: 35.08%\n","\t Val. Loss: 2.232 |  Val. Acc: 32.48%\n","Test Loss: 2.152 | Mejor test acc: 33.68%\n","Epoch: 01 | Epoch Time: 0m 0s\n","\tTrain Loss: 3.234 | Train Acc: 11.09%\n","\t Val. Loss: 3.249 |  Val. Acc: 9.38%\n","Epoch: 02 | Epoch Time: 0m 0s\n","\tTrain Loss: 2.879 | Train Acc: 18.40%\n","\t Val. Loss: 2.902 |  Val. Acc: 16.42%\n","Epoch: 03 | Epoch Time: 0m 0s\n","\tTrain Loss: 2.663 | Train Acc: 24.96%\n","\t Val. Loss: 2.699 |  Val. Acc: 22.75%\n","Epoch: 04 | Epoch Time: 0m 0s\n","\tTrain Loss: 2.452 | Train Acc: 29.76%\n","\t Val. Loss: 2.501 |  Val. Acc: 27.02%\n","Epoch: 05 | Epoch Time: 0m 0s\n","\tTrain Loss: 2.258 | Train Acc: 35.99%\n","\t Val. Loss: 2.314 |  Val. Acc: 31.52%\n","Epoch: 06 | Epoch Time: 0m 0s\n","\tTrain Loss: 2.084 | Train Acc: 41.55%\n","\t Val. Loss: 2.146 |  Val. Acc: 36.57%\n","Epoch: 07 | Epoch Time: 0m 0s\n","\tTrain Loss: 1.930 | Train Acc: 45.88%\n","\t Val. Loss: 1.995 |  Val. Acc: 42.52%\n","Epoch: 08 | Epoch Time: 0m 0s\n","\tTrain Loss: 1.774 | Train Acc: 50.81%\n","\t Val. Loss: 1.850 |  Val. Acc: 46.85%\n","Epoch: 09 | Epoch Time: 0m 0s\n","\tTrain Loss: 1.648 | Train Acc: 53.84%\n","\t Val. Loss: 1.740 |  Val. Acc: 50.03%\n","Epoch: 10 | Epoch Time: 0m 0s\n","\tTrain Loss: 1.528 | Train Acc: 57.49%\n","\t Val. Loss: 1.627 |  Val. Acc: 52.77%\n","Test Loss: 1.628 | Mejor test acc: 52.82%\n","Epoch: 01 | Epoch Time: 0m 0s\n","\tTrain Loss: 3.059 | Train Acc: 14.95%\n","\t Val. Loss: 3.086 |  Val. Acc: 11.71%\n","Epoch: 02 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.692 | Train Acc: 24.02%\n","\t Val. Loss: 2.743 |  Val. Acc: 21.56%\n","Epoch: 03 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.450 | Train Acc: 30.81%\n","\t Val. Loss: 2.528 |  Val. Acc: 26.13%\n","Epoch: 04 | Epoch Time: 0m 0s\n","\tTrain Loss: 2.272 | Train Acc: 35.67%\n","\t Val. Loss: 2.369 |  Val. Acc: 30.84%\n","Epoch: 05 | Epoch Time: 0m 0s\n","\tTrain Loss: 2.108 | Train Acc: 39.94%\n","\t Val. Loss: 2.201 |  Val. Acc: 36.13%\n","Epoch: 06 | Epoch Time: 0m 0s\n","\tTrain Loss: 1.926 | Train Acc: 45.83%\n","\t Val. Loss: 2.062 |  Val. Acc: 39.80%\n","Epoch: 07 | Epoch Time: 0m 0s\n","\tTrain Loss: 1.783 | Train Acc: 50.17%\n","\t Val. Loss: 1.938 |  Val. Acc: 43.50%\n","Epoch: 08 | Epoch Time: 0m 0s\n","\tTrain Loss: 1.701 | Train Acc: 51.60%\n","\t Val. Loss: 1.831 |  Val. Acc: 47.24%\n","Epoch: 09 | Epoch Time: 0m 0s\n","\tTrain Loss: 1.519 | Train Acc: 58.28%\n","\t Val. Loss: 1.703 |  Val. Acc: 51.53%\n","Epoch: 10 | Epoch Time: 0m 0s\n","\tTrain Loss: 1.391 | Train Acc: 62.83%\n","\t Val. Loss: 1.566 |  Val. Acc: 56.05%\n","Test Loss: 1.564 | Mejor test acc: 56.43%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 13238/13238 [00:02<00:00, 6026.62it/s]\n","100%|██████████| 2337/2337 [00:00<00:00, 6124.83it/s]\n","100%|██████████| 9981/9981 [00:01<00:00, 5417.33it/s]\n","/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.879 | Train Acc: 19.30%\n","\t Val. Loss: 2.924 |  Val. Acc: 17.74%\n","Epoch: 02 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.645 | Train Acc: 24.43%\n","\t Val. Loss: 2.715 |  Val. Acc: 20.71%\n","Epoch: 03 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.568 | Train Acc: 25.90%\n","\t Val. Loss: 2.701 |  Val. Acc: 21.04%\n","Epoch: 04 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.289 | Train Acc: 33.12%\n","\t Val. Loss: 2.373 |  Val. Acc: 29.39%\n","Epoch: 05 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.179 | Train Acc: 37.01%\n","\t Val. Loss: 2.256 |  Val. Acc: 33.77%\n","Epoch: 06 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.948 | Train Acc: 44.13%\n","\t Val. Loss: 2.075 |  Val. Acc: 38.64%\n","Epoch: 07 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.933 | Train Acc: 41.96%\n","\t Val. Loss: 2.029 |  Val. Acc: 38.71%\n","Epoch: 08 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.711 | Train Acc: 50.01%\n","\t Val. Loss: 1.858 |  Val. Acc: 44.55%\n","Epoch: 09 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.613 | Train Acc: 53.61%\n","\t Val. Loss: 1.772 |  Val. Acc: 48.20%\n","Epoch: 10 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.491 | Train Acc: 56.33%\n","\t Val. Loss: 1.645 |  Val. Acc: 51.56%\n","Test Loss: 1.645 | Mejor test acc: 51.66%\n","Epoch: 01 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.140 | Train Acc: 15.69%\n","\t Val. Loss: 3.158 |  Val. Acc: 12.79%\n","Epoch: 02 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.644 | Train Acc: 27.06%\n","\t Val. Loss: 2.683 |  Val. Acc: 22.76%\n","Epoch: 03 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.289 | Train Acc: 36.72%\n","\t Val. Loss: 2.347 |  Val. Acc: 31.84%\n","Epoch: 04 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.022 | Train Acc: 43.52%\n","\t Val. Loss: 2.094 |  Val. Acc: 38.54%\n","Epoch: 05 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.799 | Train Acc: 51.76%\n","\t Val. Loss: 1.862 |  Val. Acc: 46.71%\n","Epoch: 06 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.590 | Train Acc: 56.95%\n","\t Val. Loss: 1.672 |  Val. Acc: 51.78%\n","Epoch: 07 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.410 | Train Acc: 61.51%\n","\t Val. Loss: 1.508 |  Val. Acc: 57.16%\n","Epoch: 08 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.232 | Train Acc: 67.76%\n","\t Val. Loss: 1.362 |  Val. Acc: 62.48%\n","Epoch: 09 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.089 | Train Acc: 71.06%\n","\t Val. Loss: 1.233 |  Val. Acc: 66.38%\n","Epoch: 10 | Epoch Time: 0m 1s\n","\tTrain Loss: 0.978 | Train Acc: 73.64%\n","\t Val. Loss: 1.143 |  Val. Acc: 68.60%\n","Test Loss: 1.144 | Mejor test acc: 68.30%\n","Epoch: 01 | Epoch Time: 0m 1s\n","\tTrain Loss: 3.024 | Train Acc: 17.44%\n","\t Val. Loss: 3.077 |  Val. Acc: 13.86%\n","Epoch: 02 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.552 | Train Acc: 28.72%\n","\t Val. Loss: 2.611 |  Val. Acc: 24.39%\n","Epoch: 03 | Epoch Time: 0m 1s\n","\tTrain Loss: 2.219 | Train Acc: 39.88%\n","\t Val. Loss: 2.294 |  Val. Acc: 35.23%\n","Epoch: 04 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.960 | Train Acc: 47.11%\n","\t Val. Loss: 2.047 |  Val. Acc: 41.98%\n","Epoch: 05 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.726 | Train Acc: 53.76%\n","\t Val. Loss: 1.845 |  Val. Acc: 47.71%\n","Epoch: 06 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.540 | Train Acc: 58.06%\n","\t Val. Loss: 1.673 |  Val. Acc: 53.15%\n","Epoch: 07 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.378 | Train Acc: 62.47%\n","\t Val. Loss: 1.537 |  Val. Acc: 56.17%\n","Epoch: 08 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.217 | Train Acc: 67.28%\n","\t Val. Loss: 1.392 |  Val. Acc: 60.00%\n","Epoch: 09 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.136 | Train Acc: 68.25%\n","\t Val. Loss: 1.328 |  Val. Acc: 62.05%\n","Epoch: 10 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.008 | Train Acc: 72.36%\n","\t Val. Loss: 1.207 |  Val. Acc: 65.32%\n","Test Loss: 1.206 | Mejor test acc: 65.18%\n","Epoch: 01 | Epoch Time: 0m 0s\n","\tTrain Loss: 2.866 | Train Acc: 18.85%\n","\t Val. Loss: 2.912 |  Val. Acc: 15.92%\n","Epoch: 02 | Epoch Time: 0m 0s\n","\tTrain Loss: 2.632 | Train Acc: 23.92%\n","\t Val. Loss: 2.727 |  Val. Acc: 19.99%\n","Epoch: 03 | Epoch Time: 0m 0s\n","\tTrain Loss: 2.418 | Train Acc: 29.49%\n","\t Val. Loss: 2.505 |  Val. Acc: 25.75%\n","Epoch: 04 | Epoch Time: 0m 0s\n","\tTrain Loss: 2.231 | Train Acc: 33.21%\n","\t Val. Loss: 2.341 |  Val. Acc: 27.90%\n","Epoch: 05 | Epoch Time: 0m 0s\n","\tTrain Loss: 2.037 | Train Acc: 40.66%\n","\t Val. Loss: 2.162 |  Val. Acc: 34.54%\n","Epoch: 06 | Epoch Time: 0m 0s\n","\tTrain Loss: 1.981 | Train Acc: 40.72%\n","\t Val. Loss: 2.074 |  Val. Acc: 37.17%\n","Epoch: 07 | Epoch Time: 0m 0s\n","\tTrain Loss: 1.858 | Train Acc: 45.49%\n","\t Val. Loss: 1.999 |  Val. Acc: 40.37%\n","Epoch: 08 | Epoch Time: 0m 0s\n","\tTrain Loss: 1.746 | Train Acc: 48.15%\n","\t Val. Loss: 1.855 |  Val. Acc: 44.72%\n","Epoch: 09 | Epoch Time: 0m 0s\n","\tTrain Loss: 1.633 | Train Acc: 51.98%\n","\t Val. Loss: 1.779 |  Val. Acc: 46.95%\n","Epoch: 10 | Epoch Time: 0m 0s\n","\tTrain Loss: 1.521 | Train Acc: 55.68%\n","\t Val. Loss: 1.681 |  Val. Acc: 49.86%\n","Test Loss: 1.681 | Mejor test acc: 49.92%\n","Epoch: 01 | Epoch Time: 0m 0s\n","\tTrain Loss: 3.127 | Train Acc: 11.50%\n","\t Val. Loss: 3.146 |  Val. Acc: 10.24%\n","Epoch: 02 | Epoch Time: 0m 0s\n","\tTrain Loss: 2.773 | Train Acc: 21.17%\n","\t Val. Loss: 2.804 |  Val. Acc: 18.18%\n","Epoch: 03 | Epoch Time: 0m 0s\n","\tTrain Loss: 2.520 | Train Acc: 27.26%\n","\t Val. Loss: 2.572 |  Val. Acc: 24.08%\n","Epoch: 04 | Epoch Time: 0m 0s\n","\tTrain Loss: 2.317 | Train Acc: 33.82%\n","\t Val. Loss: 2.376 |  Val. Acc: 29.43%\n","Epoch: 05 | Epoch Time: 0m 0s\n","\tTrain Loss: 2.136 | Train Acc: 39.87%\n","\t Val. Loss: 2.189 |  Val. Acc: 35.09%\n","Epoch: 06 | Epoch Time: 0m 0s\n","\tTrain Loss: 1.953 | Train Acc: 46.61%\n","\t Val. Loss: 2.020 |  Val. Acc: 42.29%\n","Epoch: 07 | Epoch Time: 0m 0s\n","\tTrain Loss: 1.786 | Train Acc: 52.82%\n","\t Val. Loss: 1.856 |  Val. Acc: 48.91%\n","Epoch: 08 | Epoch Time: 0m 0s\n","\tTrain Loss: 1.630 | Train Acc: 56.84%\n","\t Val. Loss: 1.702 |  Val. Acc: 53.36%\n","Epoch: 09 | Epoch Time: 0m 0s\n","\tTrain Loss: 1.488 | Train Acc: 61.34%\n","\t Val. Loss: 1.568 |  Val. Acc: 58.19%\n","Epoch: 10 | Epoch Time: 0m 0s\n","\tTrain Loss: 1.357 | Train Acc: 64.76%\n","\t Val. Loss: 1.446 |  Val. Acc: 61.15%\n","Test Loss: 1.447 | Mejor test acc: 60.91%\n","Epoch: 01 | Epoch Time: 0m 0s\n","\tTrain Loss: 2.973 | Train Acc: 17.16%\n","\t Val. Loss: 3.018 |  Val. Acc: 14.46%\n","Epoch: 02 | Epoch Time: 0m 0s\n","\tTrain Loss: 2.621 | Train Acc: 25.69%\n","\t Val. Loss: 2.683 |  Val. Acc: 22.29%\n","Epoch: 03 | Epoch Time: 0m 0s\n","\tTrain Loss: 2.378 | Train Acc: 33.87%\n","\t Val. Loss: 2.462 |  Val. Acc: 29.71%\n","Epoch: 04 | Epoch Time: 0m 0s\n","\tTrain Loss: 2.142 | Train Acc: 40.67%\n","\t Val. Loss: 2.225 |  Val. Acc: 35.78%\n","Epoch: 05 | Epoch Time: 0m 0s\n","\tTrain Loss: 1.963 | Train Acc: 47.11%\n","\t Val. Loss: 2.063 |  Val. Acc: 42.89%\n","Epoch: 06 | Epoch Time: 0m 0s\n","\tTrain Loss: 1.770 | Train Acc: 53.25%\n","\t Val. Loss: 1.888 |  Val. Acc: 47.64%\n","Epoch: 07 | Epoch Time: 0m 0s\n","\tTrain Loss: 1.627 | Train Acc: 56.92%\n","\t Val. Loss: 1.745 |  Val. Acc: 52.28%\n","Epoch: 08 | Epoch Time: 0m 0s\n","\tTrain Loss: 1.501 | Train Acc: 60.40%\n","\t Val. Loss: 1.630 |  Val. Acc: 55.88%\n","Epoch: 09 | Epoch Time: 0m 0s\n","\tTrain Loss: 1.355 | Train Acc: 64.58%\n","\t Val. Loss: 1.498 |  Val. Acc: 58.51%\n","Epoch: 10 | Epoch Time: 0m 0s\n","\tTrain Loss: 1.303 | Train Acc: 64.74%\n","\t Val. Loss: 1.458 |  Val. Acc: 59.48%\n","Test Loss: 1.457 | Mejor test acc: 59.48%\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"3gCj4Izf8m6_"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1TRaEHHcal8_Tg7DWbfF635C5woNeWMd9","timestamp":1671008158959}],"authorship_tag":"ABX9TyPdaATS2FIzyvRoYRGlHhpa"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}