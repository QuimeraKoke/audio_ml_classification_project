{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8q6G2JIMsmU"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "from librosa import feature\n",
        "from librosa import display\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\n",
        "!mkdir ./speech_commands/\n",
        "!tar -xf /content/speech_commands_v0.02.tar.gz -C ./speech_commands"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rxv5wK-dOlRR",
        "outputId": "c479e9e1-a7f0-468d-f4f0-94d5e0e28615"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-13 23:57:29--  http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 74.125.20.128, 2607:f8b0:400e:c07::80\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|74.125.20.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2428923189 (2.3G) [application/gzip]\n",
            "Saving to: ‘speech_commands_v0.02.tar.gz’\n",
            "\n",
            "speech_commands_v0. 100%[===================>]   2.26G   103MB/s    in 20s     \n",
            "\n",
            "2022-10-13 23:57:49 (118 MB/s) - ‘speech_commands_v0.02.tar.gz’ saved [2428923189/2428923189]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = {}\n",
        "exclude = [\"_background_noise_\"]\n",
        "BASEPATH = \"/content/speech_commands/\"\n",
        "categories = [\"one\", \"two\", \"three\"]\n",
        "LIMIT = 50\n",
        "for label in categories:\n",
        "  dataset[label] = []\n",
        "  if (not os.path.isdir(BASEPATH + label) or label in exclude):\n",
        "    continue\n",
        "  counter = 0  \n",
        "  for file in os.listdir(BASEPATH + label + \"/\"):\n",
        "    y, sr = librosa.load(BASEPATH + label + \"/\" + file, sr=16000)\n",
        "    mfccs = librosa.feature.mfcc(y=y, sr=sr, )\n",
        "    mfcc_delta = librosa.feature.delta(mfccs)\n",
        "    mfcc_delta2 = librosa.feature.delta(mfccs, order=2)\n",
        "    dataset[label].append({\n",
        "        \"signal\": y,\n",
        "        \"mfcc\": mfccs,\n",
        "        \"delta\": mfcc_delta,\n",
        "        \"delta2\": mfcc_delta2\n",
        "    })\n",
        "    counter+=1\n",
        "\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "aCQwpBYOVD35"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfs = []\n",
        "for k,v in dataset.items():\n",
        "  tmp = pd.DataFrame(v)\n",
        "  tmp['label'] = k\n",
        "  dfs.append(tmp)\n",
        "\n",
        "df = pd.concat(dfs)"
      ],
      "metadata": {
        "id": "EnTUGxBJV_ql"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_row(row):\n",
        "  fig, ax = plt.subplots(nrows=3, sharex=True, sharey=True, figsize=(15, 15))\n",
        "  img1 = display.specshow(row[\"mfcc\"], ax=ax[0], x_axis='time')\n",
        "  ax[0].set(title='MFCC')\n",
        "  ax[0].label_outer()\n",
        "  img2 = display.specshow(row[\"delta\"], ax=ax[1], x_axis='time')\n",
        "  ax[1].set(title=r'MFCC-$\\Delta$')\n",
        "  ax[1].label_outer()\n",
        "  img3 = display.specshow(row[\"delta2\"], ax=ax[2], x_axis='time')\n",
        "  ax[2].set(title=r'MFCC-$\\Delta^2$')\n",
        "  fig.colorbar(img1, ax=[ax[0]])\n",
        "  fig.colorbar(img2, ax=[ax[1]])\n",
        "  fig.colorbar(img3, ax=[ax[2]])\n"
      ],
      "metadata": {
        "id": "ncdfJOkO_yJN"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clasificador"
      ],
      "metadata": {
        "id": "Hu6pw2Ixqa2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "rrR2usByqcn6"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_tensor(tensor, max_size=32):\n",
        "  target = torch.zeros(20, 32)\n",
        "  _, y_shape = tensor.size()\n",
        "  cut_value = min(y_shape, max_size)\n",
        "  target[:, :cut_value] = tensor\n",
        "  return target\n"
      ],
      "metadata": {
        "id": "SNWzAcxw0xx6"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c2i={}\n",
        "i2c={}\n",
        "categories = sorted(df[\"label\"].unique())\n",
        "for i, category in enumerate(categories):\n",
        "  c2i[category]=i\n",
        "  i2c[i]=category\n",
        "\n",
        "\n",
        "class SCData(Dataset):\n",
        "  def __init__(self, df, labels, c2i, i2c):\n",
        "    self.df = df\n",
        "    self.data = []\n",
        "    self.labels = []\n",
        "    self.caegories = labels\n",
        "    self.c2i = c2i\n",
        "    self.i2c = i2c\n",
        "    for ind in tqdm(range(len(df))):\n",
        "      row = df.iloc[ind]\n",
        "      \n",
        "      mfcc = pad_tensor(torch.Tensor(row['mfcc']))\n",
        "      delta = pad_tensor(torch.Tensor(row['delta']))\n",
        "      delta2 = pad_tensor(torch.Tensor(row['delta2']))\n",
        "\n",
        "      self.data.append(torch.cat((mfcc , delta, delta2), 0))\n",
        "      self.labels.append(self.c2i[row['label']])\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  def __getitem__(self, idx):\n",
        "    return self.data[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "UYOhyckkqm3R"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# train_df, test_df = train_test_split(df, test_size=0.25, random_state=42, shuffle=True)\n",
        "train, validation, test = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])"
      ],
      "metadata": {
        "id": "kpa7q2W6tzYj"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = SCData(train, categories, c2i, i2c)\n",
        "test_data = SCData(test, categories, c2i, i2c)\n",
        "valid_data = SCData(validation, categories, c2i, i2c)\n",
        "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
        "valid_loader = DataLoader(valid_data, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "id": "0Wsfs2bqs1Cl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50b683a0-e9e0-43a3-995a-1da1a80d23cd"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6898/6898 [00:01<00:00, 3451.01it/s]\n",
            "100%|██████████| 2300/2300 [00:01<00:00, 1779.51it/s]\n",
            "100%|██████████| 2299/2299 [00:00<00:00, 4775.50it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "\n",
        "        out, _ = self.rnn(x, h0)  \n",
        "        out = out[:, -1, :]\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "dQYQst7RA8EO"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
        "\n",
        "        out, _ = self.lstm(x, (h0,c0))  \n",
        "        out = out[:, -1, :]\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "UvJ8cmtAA9B-"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GRU(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(GRU, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
        "\n",
        "        out, _ = self.gru(x, h0)  \n",
        "        out = out[:, -1, :]\n",
        "         \n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "OQHW6rzGA_sO"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "num_classes = len(categories)\n",
        "num_epochs = 50\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "input_size = 60\n",
        "sequence_length = 32\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "\n",
        "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
        "\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (audios, labels) in enumerate(train_loader):  \n",
        "\n",
        "        audios = audios.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        outputs = model(audios)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Test\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for audios, labels in test_loader:\n",
        "        audios = audios.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(audios)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the 10000 test audios: {acc} %')"
      ],
      "metadata": {
        "id": "rTUXYQZ-tLvz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7668b518-f925-4210-94a2-0d4d5756d5e7"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Step [100/432], Loss: 1.0962\n",
            "Epoch [1/50], Step [200/432], Loss: 0.9889\n",
            "Epoch [1/50], Step [300/432], Loss: 1.0540\n",
            "Epoch [1/50], Step [400/432], Loss: 1.0333\n",
            "Epoch [2/50], Step [100/432], Loss: 0.6854\n",
            "Epoch [2/50], Step [200/432], Loss: 0.4360\n",
            "Epoch [2/50], Step [300/432], Loss: 0.6951\n",
            "Epoch [2/50], Step [400/432], Loss: 0.1760\n",
            "Epoch [3/50], Step [100/432], Loss: 0.5290\n",
            "Epoch [3/50], Step [200/432], Loss: 0.2574\n",
            "Epoch [3/50], Step [300/432], Loss: 0.2852\n",
            "Epoch [3/50], Step [400/432], Loss: 0.7481\n",
            "Epoch [4/50], Step [100/432], Loss: 0.0783\n",
            "Epoch [4/50], Step [200/432], Loss: 0.4697\n",
            "Epoch [4/50], Step [300/432], Loss: 0.3167\n",
            "Epoch [4/50], Step [400/432], Loss: 0.1192\n",
            "Epoch [5/50], Step [100/432], Loss: 0.0149\n",
            "Epoch [5/50], Step [200/432], Loss: 0.0676\n",
            "Epoch [5/50], Step [300/432], Loss: 0.4225\n",
            "Epoch [5/50], Step [400/432], Loss: 0.0568\n",
            "Epoch [6/50], Step [100/432], Loss: 0.1706\n",
            "Epoch [6/50], Step [200/432], Loss: 0.2514\n",
            "Epoch [6/50], Step [300/432], Loss: 0.0427\n",
            "Epoch [6/50], Step [400/432], Loss: 0.0881\n",
            "Epoch [7/50], Step [100/432], Loss: 0.0513\n",
            "Epoch [7/50], Step [200/432], Loss: 0.2420\n",
            "Epoch [7/50], Step [300/432], Loss: 0.0159\n",
            "Epoch [7/50], Step [400/432], Loss: 0.4535\n",
            "Epoch [8/50], Step [100/432], Loss: 0.0654\n",
            "Epoch [8/50], Step [200/432], Loss: 0.0818\n",
            "Epoch [8/50], Step [300/432], Loss: 0.0550\n",
            "Epoch [8/50], Step [400/432], Loss: 0.1326\n",
            "Epoch [9/50], Step [100/432], Loss: 0.1123\n",
            "Epoch [9/50], Step [200/432], Loss: 0.2879\n",
            "Epoch [9/50], Step [300/432], Loss: 0.1188\n",
            "Epoch [9/50], Step [400/432], Loss: 0.3409\n",
            "Epoch [10/50], Step [100/432], Loss: 0.0054\n",
            "Epoch [10/50], Step [200/432], Loss: 0.1184\n",
            "Epoch [10/50], Step [300/432], Loss: 0.1725\n",
            "Epoch [10/50], Step [400/432], Loss: 0.1003\n",
            "Epoch [11/50], Step [100/432], Loss: 0.0025\n",
            "Epoch [11/50], Step [200/432], Loss: 0.0246\n",
            "Epoch [11/50], Step [300/432], Loss: 0.0070\n",
            "Epoch [11/50], Step [400/432], Loss: 0.0539\n",
            "Epoch [12/50], Step [100/432], Loss: 0.0422\n",
            "Epoch [12/50], Step [200/432], Loss: 0.0361\n",
            "Epoch [12/50], Step [300/432], Loss: 0.0957\n",
            "Epoch [12/50], Step [400/432], Loss: 0.2285\n",
            "Epoch [13/50], Step [100/432], Loss: 0.0085\n",
            "Epoch [13/50], Step [200/432], Loss: 0.0586\n",
            "Epoch [13/50], Step [300/432], Loss: 0.0173\n",
            "Epoch [13/50], Step [400/432], Loss: 0.3862\n",
            "Epoch [14/50], Step [100/432], Loss: 0.0159\n",
            "Epoch [14/50], Step [200/432], Loss: 0.1211\n",
            "Epoch [14/50], Step [300/432], Loss: 0.0071\n",
            "Epoch [14/50], Step [400/432], Loss: 0.3711\n",
            "Epoch [15/50], Step [100/432], Loss: 0.0088\n",
            "Epoch [15/50], Step [200/432], Loss: 0.2104\n",
            "Epoch [15/50], Step [300/432], Loss: 0.1529\n",
            "Epoch [15/50], Step [400/432], Loss: 0.0149\n",
            "Epoch [16/50], Step [100/432], Loss: 0.0066\n",
            "Epoch [16/50], Step [200/432], Loss: 0.0095\n",
            "Epoch [16/50], Step [300/432], Loss: 0.0594\n",
            "Epoch [16/50], Step [400/432], Loss: 0.0061\n",
            "Epoch [17/50], Step [100/432], Loss: 0.0044\n",
            "Epoch [17/50], Step [200/432], Loss: 0.1058\n",
            "Epoch [17/50], Step [300/432], Loss: 0.1459\n",
            "Epoch [17/50], Step [400/432], Loss: 0.1489\n",
            "Epoch [18/50], Step [100/432], Loss: 0.0114\n",
            "Epoch [18/50], Step [200/432], Loss: 0.0024\n",
            "Epoch [18/50], Step [300/432], Loss: 0.0670\n",
            "Epoch [18/50], Step [400/432], Loss: 0.0830\n",
            "Epoch [19/50], Step [100/432], Loss: 0.0502\n",
            "Epoch [19/50], Step [200/432], Loss: 0.0469\n",
            "Epoch [19/50], Step [300/432], Loss: 0.0151\n",
            "Epoch [19/50], Step [400/432], Loss: 0.0033\n",
            "Epoch [20/50], Step [100/432], Loss: 0.0338\n",
            "Epoch [20/50], Step [200/432], Loss: 0.1539\n",
            "Epoch [20/50], Step [300/432], Loss: 0.0101\n",
            "Epoch [20/50], Step [400/432], Loss: 0.1214\n",
            "Epoch [21/50], Step [100/432], Loss: 0.0018\n",
            "Epoch [21/50], Step [200/432], Loss: 0.0153\n",
            "Epoch [21/50], Step [300/432], Loss: 0.1454\n",
            "Epoch [21/50], Step [400/432], Loss: 0.0608\n",
            "Epoch [22/50], Step [100/432], Loss: 0.0178\n",
            "Epoch [22/50], Step [200/432], Loss: 0.0033\n",
            "Epoch [22/50], Step [300/432], Loss: 0.1039\n",
            "Epoch [22/50], Step [400/432], Loss: 0.2204\n",
            "Epoch [23/50], Step [100/432], Loss: 0.0554\n",
            "Epoch [23/50], Step [200/432], Loss: 0.0015\n",
            "Epoch [23/50], Step [300/432], Loss: 0.0751\n",
            "Epoch [23/50], Step [400/432], Loss: 0.0533\n",
            "Epoch [24/50], Step [100/432], Loss: 0.0133\n",
            "Epoch [24/50], Step [200/432], Loss: 0.0189\n",
            "Epoch [24/50], Step [300/432], Loss: 0.0161\n",
            "Epoch [24/50], Step [400/432], Loss: 0.0392\n",
            "Epoch [25/50], Step [100/432], Loss: 0.2332\n",
            "Epoch [25/50], Step [200/432], Loss: 0.5043\n",
            "Epoch [25/50], Step [300/432], Loss: 0.0451\n",
            "Epoch [25/50], Step [400/432], Loss: 0.0132\n",
            "Epoch [26/50], Step [100/432], Loss: 0.0414\n",
            "Epoch [26/50], Step [200/432], Loss: 0.0030\n",
            "Epoch [26/50], Step [300/432], Loss: 0.0547\n",
            "Epoch [26/50], Step [400/432], Loss: 0.0099\n",
            "Epoch [27/50], Step [100/432], Loss: 0.0086\n",
            "Epoch [27/50], Step [200/432], Loss: 0.3174\n",
            "Epoch [27/50], Step [300/432], Loss: 0.0726\n",
            "Epoch [27/50], Step [400/432], Loss: 0.1598\n",
            "Epoch [28/50], Step [100/432], Loss: 0.0721\n",
            "Epoch [28/50], Step [200/432], Loss: 0.0027\n",
            "Epoch [28/50], Step [300/432], Loss: 0.2845\n",
            "Epoch [28/50], Step [400/432], Loss: 0.0454\n",
            "Epoch [29/50], Step [100/432], Loss: 0.0509\n",
            "Epoch [29/50], Step [200/432], Loss: 0.1698\n",
            "Epoch [29/50], Step [300/432], Loss: 0.0067\n",
            "Epoch [29/50], Step [400/432], Loss: 0.0024\n",
            "Epoch [30/50], Step [100/432], Loss: 0.0318\n",
            "Epoch [30/50], Step [200/432], Loss: 0.1514\n",
            "Epoch [30/50], Step [300/432], Loss: 0.0718\n",
            "Epoch [30/50], Step [400/432], Loss: 0.0963\n",
            "Epoch [31/50], Step [100/432], Loss: 0.0049\n",
            "Epoch [31/50], Step [200/432], Loss: 0.1973\n",
            "Epoch [31/50], Step [300/432], Loss: 0.1911\n",
            "Epoch [31/50], Step [400/432], Loss: 0.1826\n",
            "Epoch [32/50], Step [100/432], Loss: 0.1268\n",
            "Epoch [32/50], Step [200/432], Loss: 0.0139\n",
            "Epoch [32/50], Step [300/432], Loss: 0.0726\n",
            "Epoch [32/50], Step [400/432], Loss: 0.0008\n",
            "Epoch [33/50], Step [100/432], Loss: 0.0009\n",
            "Epoch [33/50], Step [200/432], Loss: 0.0038\n",
            "Epoch [33/50], Step [300/432], Loss: 0.1473\n",
            "Epoch [33/50], Step [400/432], Loss: 0.2484\n",
            "Epoch [34/50], Step [100/432], Loss: 0.2727\n",
            "Epoch [34/50], Step [200/432], Loss: 0.0185\n",
            "Epoch [34/50], Step [300/432], Loss: 0.0244\n",
            "Epoch [34/50], Step [400/432], Loss: 0.1825\n",
            "Epoch [35/50], Step [100/432], Loss: 0.0013\n",
            "Epoch [35/50], Step [200/432], Loss: 0.0075\n",
            "Epoch [35/50], Step [300/432], Loss: 0.0221\n",
            "Epoch [35/50], Step [400/432], Loss: 0.1319\n",
            "Epoch [36/50], Step [100/432], Loss: 0.0797\n",
            "Epoch [36/50], Step [200/432], Loss: 0.3272\n",
            "Epoch [36/50], Step [300/432], Loss: 0.0068\n",
            "Epoch [36/50], Step [400/432], Loss: 0.2469\n",
            "Epoch [37/50], Step [100/432], Loss: 0.0014\n",
            "Epoch [37/50], Step [200/432], Loss: 0.1106\n",
            "Epoch [37/50], Step [300/432], Loss: 0.0051\n",
            "Epoch [37/50], Step [400/432], Loss: 0.0910\n",
            "Epoch [38/50], Step [100/432], Loss: 0.0173\n",
            "Epoch [38/50], Step [200/432], Loss: 0.0019\n",
            "Epoch [38/50], Step [300/432], Loss: 0.0076\n",
            "Epoch [38/50], Step [400/432], Loss: 0.0942\n",
            "Epoch [39/50], Step [100/432], Loss: 0.0023\n",
            "Epoch [39/50], Step [200/432], Loss: 0.1230\n",
            "Epoch [39/50], Step [300/432], Loss: 0.0193\n",
            "Epoch [39/50], Step [400/432], Loss: 0.0014\n",
            "Epoch [40/50], Step [100/432], Loss: 0.0247\n",
            "Epoch [40/50], Step [200/432], Loss: 0.0005\n",
            "Epoch [40/50], Step [300/432], Loss: 0.0283\n",
            "Epoch [40/50], Step [400/432], Loss: 0.0917\n",
            "Epoch [41/50], Step [100/432], Loss: 0.2041\n",
            "Epoch [41/50], Step [200/432], Loss: 0.0313\n",
            "Epoch [41/50], Step [300/432], Loss: 0.0396\n",
            "Epoch [41/50], Step [400/432], Loss: 0.1895\n",
            "Epoch [42/50], Step [100/432], Loss: 0.1280\n",
            "Epoch [42/50], Step [200/432], Loss: 0.1240\n",
            "Epoch [42/50], Step [300/432], Loss: 0.1211\n",
            "Epoch [42/50], Step [400/432], Loss: 0.0539\n",
            "Epoch [43/50], Step [100/432], Loss: 0.0030\n",
            "Epoch [43/50], Step [200/432], Loss: 0.0635\n",
            "Epoch [43/50], Step [300/432], Loss: 0.0038\n",
            "Epoch [43/50], Step [400/432], Loss: 0.0088\n",
            "Epoch [44/50], Step [100/432], Loss: 0.1014\n",
            "Epoch [44/50], Step [200/432], Loss: 0.0105\n",
            "Epoch [44/50], Step [300/432], Loss: 0.0160\n",
            "Epoch [44/50], Step [400/432], Loss: 0.0158\n",
            "Epoch [45/50], Step [100/432], Loss: 0.0017\n",
            "Epoch [45/50], Step [200/432], Loss: 0.0126\n",
            "Epoch [45/50], Step [300/432], Loss: 0.0057\n",
            "Epoch [45/50], Step [400/432], Loss: 0.2837\n",
            "Epoch [46/50], Step [100/432], Loss: 0.0191\n",
            "Epoch [46/50], Step [200/432], Loss: 0.0119\n",
            "Epoch [46/50], Step [300/432], Loss: 0.1526\n",
            "Epoch [46/50], Step [400/432], Loss: 0.0325\n",
            "Epoch [47/50], Step [100/432], Loss: 0.0934\n",
            "Epoch [47/50], Step [200/432], Loss: 0.0097\n",
            "Epoch [47/50], Step [300/432], Loss: 0.0076\n",
            "Epoch [47/50], Step [400/432], Loss: 0.0024\n",
            "Epoch [48/50], Step [100/432], Loss: 0.0104\n",
            "Epoch [48/50], Step [200/432], Loss: 0.0046\n",
            "Epoch [48/50], Step [300/432], Loss: 0.0084\n",
            "Epoch [48/50], Step [400/432], Loss: 0.0125\n",
            "Epoch [49/50], Step [100/432], Loss: 0.0178\n",
            "Epoch [49/50], Step [200/432], Loss: 0.1092\n",
            "Epoch [49/50], Step [300/432], Loss: 0.0675\n",
            "Epoch [49/50], Step [400/432], Loss: 0.0558\n",
            "Epoch [50/50], Step [100/432], Loss: 0.1928\n",
            "Epoch [50/50], Step [200/432], Loss: 0.0189\n",
            "Epoch [50/50], Step [300/432], Loss: 0.0136\n",
            "Epoch [50/50], Step [400/432], Loss: 0.0922\n",
            "Accuracy of the network on the 10000 test audios: 97.91243838793854 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RNN_model = model\n",
        "b = 0\n",
        "m = 0\n",
        "for i in range(1000):\n",
        "  d, l = valid_data[i]\n",
        "  y_hat = model(d.reshape(-1, sequence_length, input_size))\n",
        "  y_hat = valid_data.i2c[torch.argmax(y_hat.data).item()]\n",
        "  y = valid_data.i2c[l]\n",
        "  if y_hat == y:\n",
        "    b +=1\n",
        "  else:\n",
        "    m += 1\n",
        "  # print(\"y_hat:\", y_hat, \"y:\" , y)\n",
        "print(b, m)"
      ],
      "metadata": {
        "id": "CEVaUudpzPuR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0172351-fef6-42d2-d30d-b7e79cd9b9b9"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "904 96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "num_classes = len(categories)\n",
        "num_epochs = 50\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "input_size = 60\n",
        "sequence_length = 32\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "\n",
        "model = LSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
        "\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (audios, labels) in enumerate(train_loader):  \n",
        "\n",
        "        audios = audios.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        outputs = model(audios)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Test\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for audios, labels in test_loader:\n",
        "        audios = audios.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(audios)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the 10000 test audios: {acc} %')"
      ],
      "metadata": {
        "id": "Fzw0VeH93Wft",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce1feba4-85ef-4b06-fbaa-6334d825db9c"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Step [100/432], Loss: 0.5429\n",
            "Epoch [1/50], Step [200/432], Loss: 0.1333\n",
            "Epoch [1/50], Step [300/432], Loss: 0.0659\n",
            "Epoch [1/50], Step [400/432], Loss: 0.3097\n",
            "Epoch [2/50], Step [100/432], Loss: 0.4896\n",
            "Epoch [2/50], Step [200/432], Loss: 0.0628\n",
            "Epoch [2/50], Step [300/432], Loss: 0.0171\n",
            "Epoch [2/50], Step [400/432], Loss: 0.2995\n",
            "Epoch [3/50], Step [100/432], Loss: 0.2159\n",
            "Epoch [3/50], Step [200/432], Loss: 0.0374\n",
            "Epoch [3/50], Step [300/432], Loss: 0.0193\n",
            "Epoch [3/50], Step [400/432], Loss: 0.0516\n",
            "Epoch [4/50], Step [100/432], Loss: 0.0038\n",
            "Epoch [4/50], Step [200/432], Loss: 0.0134\n",
            "Epoch [4/50], Step [300/432], Loss: 0.0228\n",
            "Epoch [4/50], Step [400/432], Loss: 0.0043\n",
            "Epoch [5/50], Step [100/432], Loss: 0.0011\n",
            "Epoch [5/50], Step [200/432], Loss: 0.0058\n",
            "Epoch [5/50], Step [300/432], Loss: 0.0017\n",
            "Epoch [5/50], Step [400/432], Loss: 0.0445\n",
            "Epoch [6/50], Step [100/432], Loss: 0.0399\n",
            "Epoch [6/50], Step [200/432], Loss: 0.2865\n",
            "Epoch [6/50], Step [300/432], Loss: 0.0579\n",
            "Epoch [6/50], Step [400/432], Loss: 0.0336\n",
            "Epoch [7/50], Step [100/432], Loss: 0.2733\n",
            "Epoch [7/50], Step [200/432], Loss: 0.0200\n",
            "Epoch [7/50], Step [300/432], Loss: 0.0359\n",
            "Epoch [7/50], Step [400/432], Loss: 0.0086\n",
            "Epoch [8/50], Step [100/432], Loss: 0.1062\n",
            "Epoch [8/50], Step [200/432], Loss: 0.0004\n",
            "Epoch [8/50], Step [300/432], Loss: 0.1798\n",
            "Epoch [8/50], Step [400/432], Loss: 0.0094\n",
            "Epoch [9/50], Step [100/432], Loss: 0.0019\n",
            "Epoch [9/50], Step [200/432], Loss: 0.0053\n",
            "Epoch [9/50], Step [300/432], Loss: 0.0151\n",
            "Epoch [9/50], Step [400/432], Loss: 0.0097\n",
            "Epoch [10/50], Step [100/432], Loss: 0.0960\n",
            "Epoch [10/50], Step [200/432], Loss: 0.0013\n",
            "Epoch [10/50], Step [300/432], Loss: 0.0005\n",
            "Epoch [10/50], Step [400/432], Loss: 0.0015\n",
            "Epoch [11/50], Step [100/432], Loss: 0.0178\n",
            "Epoch [11/50], Step [200/432], Loss: 0.0103\n",
            "Epoch [11/50], Step [300/432], Loss: 0.0055\n",
            "Epoch [11/50], Step [400/432], Loss: 0.0226\n",
            "Epoch [12/50], Step [100/432], Loss: 0.0023\n",
            "Epoch [12/50], Step [200/432], Loss: 0.0275\n",
            "Epoch [12/50], Step [300/432], Loss: 0.0345\n",
            "Epoch [12/50], Step [400/432], Loss: 0.0435\n",
            "Epoch [13/50], Step [100/432], Loss: 0.0005\n",
            "Epoch [13/50], Step [200/432], Loss: 0.0012\n",
            "Epoch [13/50], Step [300/432], Loss: 0.0125\n",
            "Epoch [13/50], Step [400/432], Loss: 0.0012\n",
            "Epoch [14/50], Step [100/432], Loss: 0.0014\n",
            "Epoch [14/50], Step [200/432], Loss: 0.0010\n",
            "Epoch [14/50], Step [300/432], Loss: 0.0003\n",
            "Epoch [14/50], Step [400/432], Loss: 0.0006\n",
            "Epoch [15/50], Step [100/432], Loss: 0.0013\n",
            "Epoch [15/50], Step [200/432], Loss: 0.0339\n",
            "Epoch [15/50], Step [300/432], Loss: 0.0036\n",
            "Epoch [15/50], Step [400/432], Loss: 0.0029\n",
            "Epoch [16/50], Step [100/432], Loss: 0.0008\n",
            "Epoch [16/50], Step [200/432], Loss: 0.0016\n",
            "Epoch [16/50], Step [300/432], Loss: 0.0004\n",
            "Epoch [16/50], Step [400/432], Loss: 0.0013\n",
            "Epoch [17/50], Step [100/432], Loss: 0.0188\n",
            "Epoch [17/50], Step [200/432], Loss: 0.0009\n",
            "Epoch [17/50], Step [300/432], Loss: 0.0025\n",
            "Epoch [17/50], Step [400/432], Loss: 0.1406\n",
            "Epoch [18/50], Step [100/432], Loss: 0.0037\n",
            "Epoch [18/50], Step [200/432], Loss: 0.0040\n",
            "Epoch [18/50], Step [300/432], Loss: 0.0016\n",
            "Epoch [18/50], Step [400/432], Loss: 0.0001\n",
            "Epoch [19/50], Step [100/432], Loss: 0.0029\n",
            "Epoch [19/50], Step [200/432], Loss: 0.0029\n",
            "Epoch [19/50], Step [300/432], Loss: 0.0196\n",
            "Epoch [19/50], Step [400/432], Loss: 0.0115\n",
            "Epoch [20/50], Step [100/432], Loss: 0.0120\n",
            "Epoch [20/50], Step [200/432], Loss: 0.0444\n",
            "Epoch [20/50], Step [300/432], Loss: 0.0162\n",
            "Epoch [20/50], Step [400/432], Loss: 0.0001\n",
            "Epoch [21/50], Step [100/432], Loss: 0.0188\n",
            "Epoch [21/50], Step [200/432], Loss: 0.0007\n",
            "Epoch [21/50], Step [300/432], Loss: 0.0024\n",
            "Epoch [21/50], Step [400/432], Loss: 0.0026\n",
            "Epoch [22/50], Step [100/432], Loss: 0.0011\n",
            "Epoch [22/50], Step [200/432], Loss: 0.0006\n",
            "Epoch [22/50], Step [300/432], Loss: 0.0053\n",
            "Epoch [22/50], Step [400/432], Loss: 0.0149\n",
            "Epoch [23/50], Step [100/432], Loss: 0.0000\n",
            "Epoch [23/50], Step [200/432], Loss: 0.0000\n",
            "Epoch [23/50], Step [300/432], Loss: 0.0002\n",
            "Epoch [23/50], Step [400/432], Loss: 0.0021\n",
            "Epoch [24/50], Step [100/432], Loss: 0.0005\n",
            "Epoch [24/50], Step [200/432], Loss: 0.0094\n",
            "Epoch [24/50], Step [300/432], Loss: 0.0012\n",
            "Epoch [24/50], Step [400/432], Loss: 0.0074\n",
            "Epoch [25/50], Step [100/432], Loss: 0.0022\n",
            "Epoch [25/50], Step [200/432], Loss: 0.0002\n",
            "Epoch [25/50], Step [300/432], Loss: 0.0100\n",
            "Epoch [25/50], Step [400/432], Loss: 0.0063\n",
            "Epoch [26/50], Step [100/432], Loss: 0.0169\n",
            "Epoch [26/50], Step [200/432], Loss: 0.0052\n",
            "Epoch [26/50], Step [300/432], Loss: 0.0007\n",
            "Epoch [26/50], Step [400/432], Loss: 0.0001\n",
            "Epoch [27/50], Step [100/432], Loss: 0.0785\n",
            "Epoch [27/50], Step [200/432], Loss: 0.0002\n",
            "Epoch [27/50], Step [300/432], Loss: 0.0996\n",
            "Epoch [27/50], Step [400/432], Loss: 0.0009\n",
            "Epoch [28/50], Step [100/432], Loss: 0.0005\n",
            "Epoch [28/50], Step [200/432], Loss: 0.0011\n",
            "Epoch [28/50], Step [300/432], Loss: 0.0145\n",
            "Epoch [28/50], Step [400/432], Loss: 0.0028\n",
            "Epoch [29/50], Step [100/432], Loss: 0.0007\n",
            "Epoch [29/50], Step [200/432], Loss: 0.0001\n",
            "Epoch [29/50], Step [300/432], Loss: 0.0001\n",
            "Epoch [29/50], Step [400/432], Loss: 0.0001\n",
            "Epoch [30/50], Step [100/432], Loss: 0.0004\n",
            "Epoch [30/50], Step [200/432], Loss: 0.0001\n",
            "Epoch [30/50], Step [300/432], Loss: 0.0000\n",
            "Epoch [30/50], Step [400/432], Loss: 0.0001\n",
            "Epoch [31/50], Step [100/432], Loss: 0.0011\n",
            "Epoch [31/50], Step [200/432], Loss: 0.0114\n",
            "Epoch [31/50], Step [300/432], Loss: 0.0022\n",
            "Epoch [31/50], Step [400/432], Loss: 0.0009\n",
            "Epoch [32/50], Step [100/432], Loss: 0.0001\n",
            "Epoch [32/50], Step [200/432], Loss: 0.0006\n",
            "Epoch [32/50], Step [300/432], Loss: 0.0503\n",
            "Epoch [32/50], Step [400/432], Loss: 0.0002\n",
            "Epoch [33/50], Step [100/432], Loss: 0.0001\n",
            "Epoch [33/50], Step [200/432], Loss: 0.0001\n",
            "Epoch [33/50], Step [300/432], Loss: 0.0109\n",
            "Epoch [33/50], Step [400/432], Loss: 0.0118\n",
            "Epoch [34/50], Step [100/432], Loss: 0.0006\n",
            "Epoch [34/50], Step [200/432], Loss: 0.0041\n",
            "Epoch [34/50], Step [300/432], Loss: 0.0002\n",
            "Epoch [34/50], Step [400/432], Loss: 0.0021\n",
            "Epoch [35/50], Step [100/432], Loss: 0.0002\n",
            "Epoch [35/50], Step [200/432], Loss: 0.0003\n",
            "Epoch [35/50], Step [300/432], Loss: 0.0001\n",
            "Epoch [35/50], Step [400/432], Loss: 0.0004\n",
            "Epoch [36/50], Step [100/432], Loss: 0.0001\n",
            "Epoch [36/50], Step [200/432], Loss: 0.2466\n",
            "Epoch [36/50], Step [300/432], Loss: 0.0002\n",
            "Epoch [36/50], Step [400/432], Loss: 0.0001\n",
            "Epoch [37/50], Step [100/432], Loss: 0.0009\n",
            "Epoch [37/50], Step [200/432], Loss: 0.0028\n",
            "Epoch [37/50], Step [300/432], Loss: 0.0033\n",
            "Epoch [37/50], Step [400/432], Loss: 0.0002\n",
            "Epoch [38/50], Step [100/432], Loss: 0.0057\n",
            "Epoch [38/50], Step [200/432], Loss: 0.0004\n",
            "Epoch [38/50], Step [300/432], Loss: 0.0048\n",
            "Epoch [38/50], Step [400/432], Loss: 0.0016\n",
            "Epoch [39/50], Step [100/432], Loss: 0.0026\n",
            "Epoch [39/50], Step [200/432], Loss: 0.0003\n",
            "Epoch [39/50], Step [300/432], Loss: 0.0004\n",
            "Epoch [39/50], Step [400/432], Loss: 0.0020\n",
            "Epoch [40/50], Step [100/432], Loss: 0.0150\n",
            "Epoch [40/50], Step [200/432], Loss: 0.0018\n",
            "Epoch [40/50], Step [300/432], Loss: 0.0001\n",
            "Epoch [40/50], Step [400/432], Loss: 0.0000\n",
            "Epoch [41/50], Step [100/432], Loss: 0.0000\n",
            "Epoch [41/50], Step [200/432], Loss: 0.0000\n",
            "Epoch [41/50], Step [300/432], Loss: 0.0002\n",
            "Epoch [41/50], Step [400/432], Loss: 0.0008\n",
            "Epoch [42/50], Step [100/432], Loss: 0.0000\n",
            "Epoch [42/50], Step [200/432], Loss: 0.0001\n",
            "Epoch [42/50], Step [300/432], Loss: 0.0000\n",
            "Epoch [42/50], Step [400/432], Loss: 0.0001\n",
            "Epoch [43/50], Step [100/432], Loss: 0.0000\n",
            "Epoch [43/50], Step [200/432], Loss: 0.0001\n",
            "Epoch [43/50], Step [300/432], Loss: 0.0000\n",
            "Epoch [43/50], Step [400/432], Loss: 0.0000\n",
            "Epoch [44/50], Step [100/432], Loss: 0.0001\n",
            "Epoch [44/50], Step [200/432], Loss: 0.0000\n",
            "Epoch [44/50], Step [300/432], Loss: 0.0000\n",
            "Epoch [44/50], Step [400/432], Loss: 0.0000\n",
            "Epoch [45/50], Step [100/432], Loss: 0.0000\n",
            "Epoch [45/50], Step [200/432], Loss: 0.0000\n",
            "Epoch [45/50], Step [300/432], Loss: 0.0000\n",
            "Epoch [45/50], Step [400/432], Loss: 0.0000\n",
            "Epoch [46/50], Step [100/432], Loss: 0.0000\n",
            "Epoch [46/50], Step [200/432], Loss: 0.0000\n",
            "Epoch [46/50], Step [300/432], Loss: 0.0000\n",
            "Epoch [46/50], Step [400/432], Loss: 0.0000\n",
            "Epoch [47/50], Step [100/432], Loss: 0.0000\n",
            "Epoch [47/50], Step [200/432], Loss: 0.0000\n",
            "Epoch [47/50], Step [300/432], Loss: 0.0000\n",
            "Epoch [47/50], Step [400/432], Loss: 0.0000\n",
            "Epoch [48/50], Step [100/432], Loss: 0.0000\n",
            "Epoch [48/50], Step [200/432], Loss: 0.0000\n",
            "Epoch [48/50], Step [300/432], Loss: 0.0000\n",
            "Epoch [48/50], Step [400/432], Loss: 0.0000\n",
            "Epoch [49/50], Step [100/432], Loss: 0.0000\n",
            "Epoch [49/50], Step [200/432], Loss: 0.0000\n",
            "Epoch [49/50], Step [300/432], Loss: 0.0000\n",
            "Epoch [49/50], Step [400/432], Loss: 0.0000\n",
            "Epoch [50/50], Step [100/432], Loss: 0.0000\n",
            "Epoch [50/50], Step [200/432], Loss: 0.0000\n",
            "Epoch [50/50], Step [300/432], Loss: 0.0000\n",
            "Epoch [50/50], Step [400/432], Loss: 0.0001\n",
            "Accuracy of the network on the 10000 test audios: 100.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LSTM_model = model\n",
        "b = 0\n",
        "m = 0\n",
        "for i in range(1000):\n",
        "  d, l = valid_data[i]\n",
        "  y_hat = model(d.reshape(-1, sequence_length, input_size))\n",
        "  y_hat = valid_data.i2c[torch.argmax(y_hat.data).item()]\n",
        "  y = valid_data.i2c[l]\n",
        "  if y_hat == y:\n",
        "    b +=1\n",
        "  else:\n",
        "    m += 1\n",
        "  # print(\"y_hat:\", y_hat, \"y:\" , y)\n",
        "print(b, m)"
      ],
      "metadata": {
        "id": "__9vg-xL3ezd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8cc8082-c718-41bc-a335-34a3883a9cf5"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "929 71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "num_classes = len(categories)\n",
        "num_epochs = 50\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "input_size = 60\n",
        "sequence_length = 32\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "\n",
        "model = GRU(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
        "\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (audios, labels) in enumerate(train_loader):  \n",
        "\n",
        "        audios = audios.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        outputs = model(audios)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Test\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for audios, labels in test_loader:\n",
        "        audios = audios.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(audios)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the 10000 test audios: {acc} %')"
      ],
      "metadata": {
        "id": "s-RAviUf30bX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8922a8a3-21dd-407d-c6f5-63c5b28f62de"
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Step [100/432], Loss: 0.4938\n",
            "Epoch [1/50], Step [200/432], Loss: 0.2204\n",
            "Epoch [1/50], Step [300/432], Loss: 0.3470\n",
            "Epoch [1/50], Step [400/432], Loss: 0.1790\n",
            "Epoch [2/50], Step [100/432], Loss: 0.1508\n",
            "Epoch [2/50], Step [200/432], Loss: 0.1027\n",
            "Epoch [2/50], Step [300/432], Loss: 0.0536\n",
            "Epoch [2/50], Step [400/432], Loss: 0.0446\n",
            "Epoch [3/50], Step [100/432], Loss: 0.0074\n",
            "Epoch [3/50], Step [200/432], Loss: 0.1189\n",
            "Epoch [3/50], Step [300/432], Loss: 0.0058\n",
            "Epoch [3/50], Step [400/432], Loss: 0.0235\n",
            "Epoch [4/50], Step [100/432], Loss: 0.0106\n",
            "Epoch [4/50], Step [200/432], Loss: 0.0318\n",
            "Epoch [4/50], Step [300/432], Loss: 0.0186\n",
            "Epoch [4/50], Step [400/432], Loss: 0.1974\n",
            "Epoch [5/50], Step [100/432], Loss: 0.0525\n",
            "Epoch [5/50], Step [200/432], Loss: 0.2970\n",
            "Epoch [5/50], Step [300/432], Loss: 0.0152\n",
            "Epoch [5/50], Step [400/432], Loss: 0.0128\n",
            "Epoch [6/50], Step [100/432], Loss: 0.0029\n",
            "Epoch [6/50], Step [200/432], Loss: 0.5586\n",
            "Epoch [6/50], Step [300/432], Loss: 0.0129\n",
            "Epoch [6/50], Step [400/432], Loss: 0.0255\n",
            "Epoch [7/50], Step [100/432], Loss: 0.0022\n",
            "Epoch [7/50], Step [200/432], Loss: 0.0011\n",
            "Epoch [7/50], Step [300/432], Loss: 0.0459\n",
            "Epoch [7/50], Step [400/432], Loss: 0.0007\n",
            "Epoch [8/50], Step [100/432], Loss: 0.0059\n",
            "Epoch [8/50], Step [200/432], Loss: 0.0027\n",
            "Epoch [8/50], Step [300/432], Loss: 0.0019\n",
            "Epoch [8/50], Step [400/432], Loss: 0.1169\n",
            "Epoch [9/50], Step [100/432], Loss: 0.0002\n",
            "Epoch [9/50], Step [200/432], Loss: 0.0039\n",
            "Epoch [9/50], Step [300/432], Loss: 0.0083\n",
            "Epoch [9/50], Step [400/432], Loss: 0.0005\n",
            "Epoch [10/50], Step [100/432], Loss: 0.0005\n",
            "Epoch [10/50], Step [200/432], Loss: 0.0021\n",
            "Epoch [10/50], Step [300/432], Loss: 0.0572\n",
            "Epoch [10/50], Step [400/432], Loss: 0.0013\n",
            "Epoch [11/50], Step [100/432], Loss: 0.0003\n",
            "Epoch [11/50], Step [200/432], Loss: 0.0036\n",
            "Epoch [11/50], Step [300/432], Loss: 0.0024\n",
            "Epoch [11/50], Step [400/432], Loss: 0.0034\n",
            "Epoch [12/50], Step [100/432], Loss: 0.0046\n",
            "Epoch [12/50], Step [200/432], Loss: 0.0004\n",
            "Epoch [12/50], Step [300/432], Loss: 0.2472\n",
            "Epoch [12/50], Step [400/432], Loss: 0.0007\n",
            "Epoch [13/50], Step [100/432], Loss: 0.1446\n",
            "Epoch [13/50], Step [200/432], Loss: 0.0006\n",
            "Epoch [13/50], Step [300/432], Loss: 0.0013\n",
            "Epoch [13/50], Step [400/432], Loss: 0.0011\n",
            "Epoch [14/50], Step [100/432], Loss: 0.0013\n",
            "Epoch [14/50], Step [200/432], Loss: 0.0021\n",
            "Epoch [14/50], Step [300/432], Loss: 0.0103\n",
            "Epoch [14/50], Step [400/432], Loss: 0.0003\n",
            "Epoch [15/50], Step [100/432], Loss: 0.0121\n",
            "Epoch [15/50], Step [200/432], Loss: 0.0002\n",
            "Epoch [15/50], Step [300/432], Loss: 0.0424\n",
            "Epoch [15/50], Step [400/432], Loss: 0.0041\n",
            "Epoch [16/50], Step [100/432], Loss: 0.0044\n",
            "Epoch [16/50], Step [200/432], Loss: 0.0010\n",
            "Epoch [16/50], Step [300/432], Loss: 0.0164\n",
            "Epoch [16/50], Step [400/432], Loss: 0.0003\n",
            "Epoch [17/50], Step [100/432], Loss: 0.0180\n",
            "Epoch [17/50], Step [200/432], Loss: 0.0003\n",
            "Epoch [17/50], Step [300/432], Loss: 0.0056\n",
            "Epoch [17/50], Step [400/432], Loss: 0.0010\n",
            "Epoch [18/50], Step [100/432], Loss: 0.0008\n",
            "Epoch [18/50], Step [200/432], Loss: 0.0012\n",
            "Epoch [18/50], Step [300/432], Loss: 0.0101\n",
            "Epoch [18/50], Step [400/432], Loss: 0.0770\n",
            "Epoch [19/50], Step [100/432], Loss: 0.0018\n",
            "Epoch [19/50], Step [200/432], Loss: 0.0002\n",
            "Epoch [19/50], Step [300/432], Loss: 0.0026\n",
            "Epoch [19/50], Step [400/432], Loss: 0.0012\n",
            "Epoch [20/50], Step [100/432], Loss: 0.0016\n",
            "Epoch [20/50], Step [200/432], Loss: 0.0013\n",
            "Epoch [20/50], Step [300/432], Loss: 0.0102\n",
            "Epoch [20/50], Step [400/432], Loss: 0.0009\n",
            "Epoch [21/50], Step [100/432], Loss: 0.0005\n",
            "Epoch [21/50], Step [200/432], Loss: 0.0001\n",
            "Epoch [21/50], Step [300/432], Loss: 0.0003\n",
            "Epoch [21/50], Step [400/432], Loss: 0.0001\n",
            "Epoch [22/50], Step [100/432], Loss: 0.0001\n",
            "Epoch [22/50], Step [200/432], Loss: 0.0014\n",
            "Epoch [22/50], Step [300/432], Loss: 0.0000\n",
            "Epoch [22/50], Step [400/432], Loss: 0.0002\n",
            "Epoch [23/50], Step [100/432], Loss: 0.0045\n",
            "Epoch [23/50], Step [200/432], Loss: 0.0001\n",
            "Epoch [23/50], Step [300/432], Loss: 0.0156\n",
            "Epoch [23/50], Step [400/432], Loss: 0.0011\n",
            "Epoch [24/50], Step [100/432], Loss: 0.0002\n",
            "Epoch [24/50], Step [200/432], Loss: 0.0009\n",
            "Epoch [24/50], Step [300/432], Loss: 0.0050\n",
            "Epoch [24/50], Step [400/432], Loss: 0.0015\n",
            "Epoch [25/50], Step [100/432], Loss: 0.0000\n",
            "Epoch [25/50], Step [200/432], Loss: 0.0003\n",
            "Epoch [25/50], Step [300/432], Loss: 0.0001\n",
            "Epoch [25/50], Step [400/432], Loss: 0.0110\n",
            "Epoch [26/50], Step [100/432], Loss: 0.0001\n",
            "Epoch [26/50], Step [200/432], Loss: 0.0200\n",
            "Epoch [26/50], Step [300/432], Loss: 0.0010\n",
            "Epoch [26/50], Step [400/432], Loss: 0.0003\n",
            "Epoch [27/50], Step [100/432], Loss: 0.0001\n",
            "Epoch [27/50], Step [200/432], Loss: 0.0003\n",
            "Epoch [27/50], Step [300/432], Loss: 0.0006\n",
            "Epoch [27/50], Step [400/432], Loss: 0.0893\n",
            "Epoch [28/50], Step [100/432], Loss: 0.0005\n",
            "Epoch [28/50], Step [200/432], Loss: 0.0635\n",
            "Epoch [28/50], Step [300/432], Loss: 0.0092\n",
            "Epoch [28/50], Step [400/432], Loss: 0.0005\n",
            "Epoch [29/50], Step [100/432], Loss: 0.0009\n",
            "Epoch [29/50], Step [200/432], Loss: 0.0003\n",
            "Epoch [29/50], Step [300/432], Loss: 0.0011\n",
            "Epoch [29/50], Step [400/432], Loss: 0.0001\n",
            "Epoch [30/50], Step [100/432], Loss: 0.0049\n",
            "Epoch [30/50], Step [200/432], Loss: 0.0000\n",
            "Epoch [30/50], Step [300/432], Loss: 0.0240\n",
            "Epoch [30/50], Step [400/432], Loss: 0.0000\n",
            "Epoch [31/50], Step [100/432], Loss: 0.0000\n",
            "Epoch [31/50], Step [200/432], Loss: 0.0000\n",
            "Epoch [31/50], Step [300/432], Loss: 0.1021\n",
            "Epoch [31/50], Step [400/432], Loss: 0.0072\n",
            "Epoch [32/50], Step [100/432], Loss: 0.0038\n",
            "Epoch [32/50], Step [200/432], Loss: 0.0009\n",
            "Epoch [32/50], Step [300/432], Loss: 0.0750\n",
            "Epoch [32/50], Step [400/432], Loss: 0.0129\n",
            "Epoch [33/50], Step [100/432], Loss: 0.0010\n",
            "Epoch [33/50], Step [200/432], Loss: 0.0021\n",
            "Epoch [33/50], Step [300/432], Loss: 0.0036\n",
            "Epoch [33/50], Step [400/432], Loss: 0.0012\n",
            "Epoch [34/50], Step [100/432], Loss: 0.0001\n",
            "Epoch [34/50], Step [200/432], Loss: 0.0069\n",
            "Epoch [34/50], Step [300/432], Loss: 0.0803\n",
            "Epoch [34/50], Step [400/432], Loss: 0.0000\n",
            "Epoch [35/50], Step [100/432], Loss: 0.0001\n",
            "Epoch [35/50], Step [200/432], Loss: 0.0093\n",
            "Epoch [35/50], Step [300/432], Loss: 0.0011\n",
            "Epoch [35/50], Step [400/432], Loss: 0.0006\n",
            "Epoch [36/50], Step [100/432], Loss: 0.0014\n",
            "Epoch [36/50], Step [200/432], Loss: 0.0153\n",
            "Epoch [36/50], Step [300/432], Loss: 0.0121\n",
            "Epoch [36/50], Step [400/432], Loss: 0.0001\n",
            "Epoch [37/50], Step [100/432], Loss: 0.0002\n",
            "Epoch [37/50], Step [200/432], Loss: 0.0004\n",
            "Epoch [37/50], Step [300/432], Loss: 0.0004\n",
            "Epoch [37/50], Step [400/432], Loss: 0.0013\n",
            "Epoch [38/50], Step [100/432], Loss: 0.0002\n",
            "Epoch [38/50], Step [200/432], Loss: 0.0002\n",
            "Epoch [38/50], Step [300/432], Loss: 0.0058\n",
            "Epoch [38/50], Step [400/432], Loss: 0.0001\n",
            "Epoch [39/50], Step [100/432], Loss: 0.0023\n",
            "Epoch [39/50], Step [200/432], Loss: 0.0002\n",
            "Epoch [39/50], Step [300/432], Loss: 0.0001\n",
            "Epoch [39/50], Step [400/432], Loss: 0.0001\n",
            "Epoch [40/50], Step [100/432], Loss: 0.0001\n",
            "Epoch [40/50], Step [200/432], Loss: 0.0006\n",
            "Epoch [40/50], Step [300/432], Loss: 0.0001\n",
            "Epoch [40/50], Step [400/432], Loss: 0.0001\n",
            "Epoch [41/50], Step [100/432], Loss: 0.0002\n",
            "Epoch [41/50], Step [200/432], Loss: 0.0002\n",
            "Epoch [41/50], Step [300/432], Loss: 0.0000\n",
            "Epoch [41/50], Step [400/432], Loss: 0.0000\n",
            "Epoch [42/50], Step [100/432], Loss: 0.0000\n",
            "Epoch [42/50], Step [200/432], Loss: 0.0000\n",
            "Epoch [42/50], Step [300/432], Loss: 0.0000\n",
            "Epoch [42/50], Step [400/432], Loss: 0.0000\n",
            "Epoch [43/50], Step [100/432], Loss: 0.0000\n",
            "Epoch [43/50], Step [200/432], Loss: 0.0000\n",
            "Epoch [43/50], Step [300/432], Loss: 0.0000\n",
            "Epoch [43/50], Step [400/432], Loss: 0.0000\n",
            "Epoch [44/50], Step [100/432], Loss: 0.0001\n",
            "Epoch [44/50], Step [200/432], Loss: 0.0000\n",
            "Epoch [44/50], Step [300/432], Loss: 0.0001\n",
            "Epoch [44/50], Step [400/432], Loss: 0.0001\n",
            "Epoch [45/50], Step [100/432], Loss: 0.0000\n",
            "Epoch [45/50], Step [200/432], Loss: 0.0000\n",
            "Epoch [45/50], Step [300/432], Loss: 0.0000\n",
            "Epoch [45/50], Step [400/432], Loss: 0.0000\n",
            "Epoch [46/50], Step [100/432], Loss: 0.0000\n",
            "Epoch [46/50], Step [200/432], Loss: 0.0000\n",
            "Epoch [46/50], Step [300/432], Loss: 0.0000\n",
            "Epoch [46/50], Step [400/432], Loss: 0.0000\n",
            "Epoch [47/50], Step [100/432], Loss: 0.0000\n",
            "Epoch [47/50], Step [200/432], Loss: 0.0000\n",
            "Epoch [47/50], Step [300/432], Loss: 0.0000\n",
            "Epoch [47/50], Step [400/432], Loss: 0.0000\n",
            "Epoch [48/50], Step [100/432], Loss: 0.0000\n",
            "Epoch [48/50], Step [200/432], Loss: 0.0000\n",
            "Epoch [48/50], Step [300/432], Loss: 0.0000\n",
            "Epoch [48/50], Step [400/432], Loss: 0.0000\n",
            "Epoch [49/50], Step [100/432], Loss: 0.0000\n",
            "Epoch [49/50], Step [200/432], Loss: 0.0000\n",
            "Epoch [49/50], Step [300/432], Loss: 0.0000\n",
            "Epoch [49/50], Step [400/432], Loss: 0.0000\n",
            "Epoch [50/50], Step [100/432], Loss: 0.0000\n",
            "Epoch [50/50], Step [200/432], Loss: 0.0000\n",
            "Epoch [50/50], Step [300/432], Loss: 0.0000\n",
            "Epoch [50/50], Step [400/432], Loss: 0.0000\n",
            "Accuracy of the network on the 10000 test audios: 100.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GRU_model = model\n",
        "b = 0\n",
        "m = 0\n",
        "for i in range(1000):\n",
        "  d, l = valid_data[i]\n",
        "  y_hat = model(d.reshape(-1, sequence_length, input_size))\n",
        "  y_hat = valid_data.i2c[torch.argmax(y_hat.data).item()]\n",
        "  y = valid_data.i2c[l]\n",
        "  if y_hat == y:\n",
        "    b +=1\n",
        "  else:\n",
        "    m += 1\n",
        "  # print(\"y_hat:\", y_hat, \"y:\" , y)\n",
        "print(b, m)"
      ],
      "metadata": {
        "id": "uKQA9KdmB2us",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3e51e73-6825-401a-9ff0-3db42867ab59"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "927 73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(RNN_model, \"/content/rnn.pt\")\n",
        "torch.save(LSTM_model, \"/content/lstm.pt\")\n",
        "torch.save(GRU_model, \"/content/gru.pt\")\n"
      ],
      "metadata": {
        "id": "dEw5J8esY5Mp"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rj4F6Rv2iTiz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}